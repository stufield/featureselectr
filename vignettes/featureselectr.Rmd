---
title: "Introduction to featureselectr"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Introduction to featureselectr}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
library(featureselectr)
library(ggplot2)
knitr::opts_chunk$set(
  collapse = TRUE,
  comment  = "#>",
  fig.width  = 10,
  fig.height = 5
)
simdata <- wranglr::simdata
options(rlib_message_verbosity = "quiet")   # silence notch warning boxplots
options(width = 70L)
```


The `featureselectr` package is an object oriented package containing
functionality designed for feature selection, model building,
and/or classifier development.


--------------


## Two primary functions in `featureselectr`

* `feature_selection()`
  + Sets up the feature selection object containing all
    search information.
* `Search()`
  + Performs the actual search.


----------------------


### Search Type Helpers

There are *two* main search types to choose from. See `?search_type`.

```{r search-type}
search_type_forward_model()   # Forward stepwise

search_type_backward_model()  # Backward stepwise
```

### Model Type Helpers

There are *three* main model types to choose from. See `?model_type`.

```{r model-type}
model_type_lr()  # Logistic regression

model_type_lm()  # Linear regression

model_type_nb()  # Naive Bayes
```

### Cost Helpers

There are *five* available cost functions, that the used typically does not
need to call directly. Simply pass one of the following as a *string* to
the `cost =` argument to `feature_selection()`.
See `?feature_selection` and perhaps `?cost`.

* **AUC**: Area under the curve (classification)
* **CCC**: Concordance Correlation Coefficient (regression)
* **MSE**: Mean-squared Error (regression)
* **R2**: R-squared (regression)
* **sens/spec**: Sensitivity `+` Specificity (sum; classification)



-----------------


## Feature Selection with Naive Bayes

The analysis below is performed with the simulated data set from
`wranglr::simdata`. We fit a Naive Bayes model during the
feature selection. The setup below specifies **3** independent runs
of **5** fold cross-validation.

Higher folds might generate slightly different results, but a **20-25%**
hold-out is fairly common. Of course, more runs (repeats) will take longer.
There are `r length(attributes(data)$sig_feats$class)` features that should
be significant in a binary classification context. They are identified in
the attributes of the object itself. We will restrict the search to the
top **15** steps (there are
`r length(helpr:::get_analytes(simdata))`
total features; thus approx. 35 false positives).


### Setup `feature_select` Object

```{r fs-setup-nb}
data <- simdata

# True positive features
attributes(data)$sig_feats$class

# log-transform, center, and scale
cs <- function(x) {
  out <- log10(x)
  out <- out - mean(out)
  out / sd(out)
}

# scramble order of feats random
feats <- withr::with_seed(123, sample(helpr:::get_analytes(data)))
data[, feats] <- apply(data[, feats], 2, cs)

# set model type and column name of response variable
mt <- model_type_nb(response = "class_response")

# set search method function to 'forward' and 'model'
# restrict to the top 10 steps in the search; then stop
sm <- search_type_forward_model(max_steps = 10L)

# setup feature selection object
fs_setup <- feature_selection(
  data,
  candidate_features = feats,
  model_type  = mt,
  search_type = sm,
  runs  = 3L,
  folds = 5L,
  cost  = "AUC",
  random_seed = 1
)

fs_setup
```


### Perform the Search

The S3 method `Search()` performs the actual feature selection,
and method dispatch occurs depending on the class of `fs_nb`.

```{r search-fs-nb, eval = FALSE}
fs_nb <- Search(fs_setup)
```

```{r load-fs-nb, echo = FALSE}
fs_nb <- readRDS("fs_nb_obj.rds")   # secretly load
```


### Plot the Selection Paths

There is an S3 `plot()` method easily visualizes the steps of
the selection algorithm, and highlights the peak (AUC) and the
models at $1\sigma$ and $2\sigma$ from the peak.
The 2 panels show a distribution-free representation of the data
(left; Wilcoxon signed-ranks with medians) and a distribution dependent
representation (right; standard errors with means and CI95%).

```{r plot-fs-nb, out.width = "100%"}
plot(fs_nb)
```


---------------


## Logistic Regression

We can use the `update()` method to modify the existing
`feature_select` object.

```{r update-fs-setup}
fs_update <- update(
  fs_setup,   # the `feature_selection` object being modified
  model_type  = model_type_lr(response = "class_response"), # logistic reg
  search_type = search_type_forward_model(max_steps = 15L), # increase max steps
  stratified  = TRUE   # now stratify
)

fs_update
```

### Perform the Search

```{r search-lr, eval = FALSE}
fs_lr <- Search(fs_update)
```

```{r load-fs-lr, echo = FALSE}
fs_lr <- readRDS("fs_lr_obj.rds")   # secretly load
```

### Plot the Selection Algorithm

```{r plot-fs-lr, out.width = "100%"}
plot(fs_lr)
```

