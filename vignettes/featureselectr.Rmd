---
title: "Introduction to featureselectr"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Introduction to featureselectr}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
library(featureselectr)
library(ggplot2)
knitr::opts_chunk$set(
  collapse = TRUE, comment = "#>"
)
sim_adat <- splyr::sim_adat
```


The `featureselectr` package contains functionality designed for feature 
selection, model building, and/or classifier development.


--------------


## Useful functions in `featureselectr`

* `featureSelection()`
  + Set up the feature selection object containing all search information.
* `Search()`
  + Perform the actual search.


----------------------


### Search type helper functions
There are _two_ main search types to choose from. See `?search_type`.

```{r search-type}
search_type_forward_model()   # Forward stepwise

search_type_backward_model()  # Backward stepwise
```

### Model Type helper functions

There are _three_ main model types to choose from. See `?model_type`.

```{r model-type}
model_type_glm()  # Logistic regression

model_type_lm()   # Linear regression

model_type_nb()   # Naive Bayes
```

### Cost functions

There are _five_ available cost functions, that the used typically does not
need to call directly. Simply pass one of the following as a _string_ to 
the `cost =` argument to `featureselection()`.
See `?featureSelection` and perhaps `?cost`.

* **AUC**: Area under the curve (classification)
* **CCC**: Concordance Correlation Coefficient (regression)
* **MSE**: Mean-squared Error (regression)
* **R2**: R-squared (regression)
* **sens/spec**: Sensitivity `+` Specificity (sum; classification)



-----------------


## Feature Selection for Naive Bayes

The analysis below is performed with the simulated data set from
`splyr::sim_adat`. We will fit a naive Bayes model during the
feature selection. The setup below specifies **5** independent runs 
of **4** fold cross-validation.
Higher folds might generate slightly different results, but a **20-25%**
hold-out is fairly common. Of course, more runs (repeats) will take longer.
There are `r length(attributes(data)$sig_feats$class)` features that should 
be significant in a binary classification context. They are identified in 
the attributes of the object itself. We will restrict the search to the 
top **15** steps (there are
`r length(featureselectr:::get_analytes(sim_adat))`
total features; thus approx. 35 false positives).

### Setup `FS` object

```{r naive_bayes_setup}
# True positive features
data <- sim_adat
attributes(data)$sig_feats$class

# log-transform, center, and scale
cs <- function(x) {
  out <- log10(x)
  out <- out - mean(out)
  out / sd(out)
}
feats <- featureselectr:::get_analytes(data)
data[, feats] <- apply(data[, feats], 2, cs)

# set model type and column name of response variable
mt <- model_type_nb(response = "class_response")

# set search method function to 'forward' and 'model'
# restrict to the top 10 steps in the search; then stop
sm <- search_type_forward_model(max_steps = 15L)

# setup feature selection object
fs_setup <- feature_selection(data, candidate_markers = feats, model_type = mt,
                              search_type = sm, runs = 5, folds = 4,
                              cost = "AUC", random_seed = 1)

class(fs_setup)
# print setup object
fs_setup
```

### Perform the search

The S3 method `Search()` performs the actual feature selection,
and method dispatch occurs depending on the class of `fs_nb`.

```{r search-nb, eval = FALSE, message = FALSE}
fs_nb <- Search(fs_setup)
```

```{r load_fs, echo = FALSE}
fs_nb <- readRDS("fs_nb_obj.rds")   # load this secretly
```

### Plot the feature selections

A S3 `plot()` method easily visualizes the steps of the selection,
and highlights the peak (AUC) and the models at
1$\sigma$ and 2$\sigma$ from the peak.
The 2 panels show a distribution-free representation of the data 
(left; median; Wilcoxon signed-ranks) and a distribution dependent
representation (right; means; standard errors).

```{r plot-nb, warning = FALSE, message = FALSE, out.width = "100%", fig.width = 10, fig.height = 5}
plot(fs_nb)
```


---------------


## Feature Selection with Logistic Regression

We can use the `update()` function to modify the existing `feature_select`
object.

```{r update}
# True positive features
attributes(sim_adat)$sig_feats$class

fs_update <- update(
  fs_setup,
  model_type = model_type_glm(response = "class_response"),
  stratified = TRUE
)
                    
class(fs_update)

# print method
fs_update
```

### Perform the search

```{r search_lr, eval = FALSE, message = FALSE}
fs_lr <- Search(fs_update)
```

```{r load_fs2, echo = FALSE}
fs_lr <- readRDS("fs_lr_obj.rds")   # load this secretly
```

### Plot the feature selections

```{r plot_lr, warning = FALSE, message = FALSE, out.width = "100%", fig.width = 10, fig.height = 5}
plot(fs_lr)
```

---------------
