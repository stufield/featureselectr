---
title: "Introduction to featureselectr"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Introduction to featureselectr}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
library(featureselectr)
library(ggplot2)
knitr::opts_chunk$set(
  collapse = TRUE, comment = "#>"
)
simdata <- wranglr::simdata
```


The `featureselectr` package contains functionality designed for feature 
selection, model building, and/or classifier development.


--------------


## Two primary functions in `featureselectr`

* `feature_selection()`
  + Set up the feature selection object containing all search information.
* `Search()`
  + Perform the actual search.


----------------------


### Search Type Helpers

There are *two* main search types to choose from. See `?search_type`.

```{r search-type}
search_type_forward_model()   # Forward stepwise

search_type_backward_model()  # Backward stepwise
```

### Model Type Helpers

There are _three_ main model types to choose from. See `?model_type`.

```{r model-type}
model_type_lr()  # Logistic regression

model_type_lm()   # Linear regression

model_type_nb()   # Naive Bayes
```

### Cost Helpers

There are _five_ available cost functions, that the used typically does not
need to call directly. Simply pass one of the following as a _string_ to 
the `cost =` argument to `featureselection()`.
See `?featureSelection` and perhaps `?cost`.

* **AUC**: Area under the curve (classification)
* **CCC**: Concordance Correlation Coefficient (regression)
* **MSE**: Mean-squared Error (regression)
* **R2**: R-squared (regression)
* **sens/spec**: Sensitivity `+` Specificity (sum; classification)



-----------------


## Feature Selection for Naive Bayes

The analysis below is performed with the simulated data set from
`wranglr::simdata`. We will fit a naive Bayes model during the
feature selection. The setup below specifies **5** independent runs 
of **4** fold cross-validation.
Higher folds might generate slightly different results, but a **20-25%**
hold-out is fairly common. Of course, more runs (repeats) will take longer.
There are `r length(attributes(data)$sig_feats$class)` features that should 
be significant in a binary classification context. They are identified in 
the attributes of the object itself. We will restrict the search to the 
top **15** steps (there are
`r length(helpr:::get_analytes(simdata))`
total features; thus approx. 35 false positives).

### Setup `FS` object

```{r fs-setup-nb}
data <- simdata

# True positive features
attributes(data)$sig_feats$class

# log-transform, center, and scale
cs <- function(x) {
  out <- log10(x)
  out <- out - mean(out)
  out / sd(out)
}

# scramble order of feats random
feats <- withr::with_seed(123, sample(helpr:::get_analytes(data)))
data[, feats] <- apply(data[, feats], 2, cs)

# set model type and column name of response variable
mt <- model_type_nb(response = "class_response")

# set search method function to 'forward' and 'model'
# restrict to the top 10 steps in the search; then stop
sm <- search_type_forward_model(max_steps = 10L)

# setup feature selection object
fs_setup <- feature_selection(data, candidate_features = feats,
                              model_type = mt, search_type = sm,
                              runs = 3, folds = 5,
                              cost = "AUC", random_seed = 1)

fs_setup
```

### Perform the search

The S3 method `Search()` performs the actual feature selection,
and method dispatch occurs depending on the class of `fs_nb`.

```{r search-fs-nb, eval = FALSE}
fs_nb <- Search(fs_setup)
```

```{r load-fs-nb, echo = FALSE}
fs_nb <- readRDS("fs_nb_obj.rds")   # load this secretly
```

### Plot the Selection Paths

A S3 `plot()` method easily visualizes the steps of the selection,
and highlights the peak (AUC) and the models at
1$\sigma$ and 2$\sigma$ from the peak.
The 2 panels show a distribution-free representation of the data 
(left; median; Wilcoxon signed-ranks) and a distribution dependent
representation (right; means; standard errors).

```{r plot-fs-nb, warning = FALSE, out.width = "100%", fig.width = 10, fig.height = 5}
plot(fs_nb)
```


---------------


## Feature Selection with Logistic Regression

We can use the `update()` function to modify the existing
`feature_select` object.

```{r update-fs-setup}
fs_update <- update(
  fs_setup,
  model_type = model_type_lr(response = "class_response"),  # use logistic reg
  search_type = search_type_forward_model(max_steps = 15L), # increase steps
  stratified = TRUE    # stratify
)
                    
fs_update
```

### Perform the search

```{r search-lr, eval = FALSE}
fs_lr <- Search(fs_update)
```

```{r load-fs-lr, echo = FALSE}
fs_lr <- readRDS("fs_lr_obj.rds")   # load this secretly
```

### Plot the feature selections

```{r plot-fs-lr, warning = FALSE, out.width = "100%", fig.width = 10, fig.height = 5}
plot(fs_lr)
```

---------------
